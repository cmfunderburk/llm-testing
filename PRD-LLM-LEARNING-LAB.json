{
  "task": {
    "name": "LLM Learning Lab",
    "description": "A hands-on learning environment to build deep intuition about how LLMs work through instrumented experiments across fine-tuning mechanics, attention visualization, representation probing, and paper reproduction. Success is measured by mental model formation, paper-reading fluency, and experimental intuition.",
    "created": "2025-01-07T12:00:00Z",
    "vision_source": "VISION.md",
    "completion_promise": "<promise>LLM-LEARNING-LAB-COMPLETE</promise>"
  },

  "context": {
    "codebase_state": "active",
    "primary_language": "python",
    "frameworks": ["unsloth", "transformers", "trl", "datasets", "torch"],
    "hardware": "NVIDIA RTX 5060 Ti 16GB (Blackwell)",
    "vision_extraction": {
      "primary_goal": "Build a mental model of LLMs deep enough to read research papers fluently, understand training dynamics, make informed fine-tuning decisions, and determine whether hardware investment is worthwhile",
      "success_criteria": [
        "Mental model formed: Can explain transformer architecture, attention mechanics, and training dynamics without referencing materials",
        "Paper-reading fluency: Can read 'Bayesian Geometry of Transformer Attention' and follow methodology/claims",
        "Experimental intuition: Can hypothesize what an experiment will show before running it, understand why when results differ"
      ],
      "deliverables": [
        "Working code with inline explanatory notes",
        "Structured documentation in docs/concepts/",
        "Questions log (central QUESTIONS.md + per-experiment)",
        "Self-assessment docs for each major concept",
        "Retrospective reflection docs after each track",
        "Paper summary in own words + reproduction experiments"
      ]
    },
    "gap_analysis_summary": "Existing qwen-finetune/ has working QLoRA script but is production-focused, not learning-focused. No attention visualization, representation probing, or paper reproduction code exists. No docs/concepts/ structure, no questions log, no experiment organization."
  },

  "phases": [
    {
      "phase": 1,
      "name": "Infrastructure & Foundation",
      "description": "Refactor codebase to learning-focused structure. Establish documentation framework, questions log, and experiment organization. Prepare Track A infrastructure.",
      "features": ["FEAT-001", "FEAT-002", "FEAT-003", "FEAT-004"],
      "entry_criteria": "Vision document exists, basic QLoRA script works",
      "exit_criteria": "Codebase restructured, docs/concepts/ exists, QUESTIONS.md initialized, first instrumented experiment ready to run",
      "status": "complete"
    },
    {
      "phase": 2,
      "name": "Track A: Fine-Tuning Mechanics",
      "description": "Instrumented experiments exploring training dynamics: loss curves, learning rate effects, LoRA rank comparisons, catastrophic forgetting. Answer the question: what is the minimum interesting experiment?",
      "features": ["FEAT-005", "FEAT-006", "FEAT-007", "FEAT-008", "FEAT-009"],
      "entry_criteria": "Phase 1 complete, instrumented training script ready",
      "exit_criteria": "At least 3 documented experiments with hypotheses and learnings, self-assessment doc on training dynamics, 'minimum interesting experiment' question answered",
      "status": "complete"
    },
    {
      "phase": 3,
      "name": "Track B: Attention Visualization",
      "description": "Build tooling to visualize attention patterns. Examine patterns across layers, compare base vs fine-tuned models, observe how attention changes during training.",
      "features": ["FEAT-010", "FEAT-011", "FEAT-012", "FEAT-013"],
      "entry_criteria": "Phase 2 complete, trained model artifacts available for comparison",
      "exit_criteria": "Attention visualization tooling works, documented experiments comparing attention patterns, self-assessment doc on attention mechanics",
      "status": "complete"
    },
    {
      "phase": 4,
      "name": "Track C: Representation Probing",
      "description": "Examine residual stream contents. Extract and analyze activations, track layer-by-layer transformations, explore feature representations.",
      "features": ["FEAT-014", "FEAT-015", "FEAT-016"],
      "entry_criteria": "Phase 3 complete, attention intuition established",
      "exit_criteria": "Activation extraction tooling works, documented probing experiments, self-assessment doc on representations",
      "status": "pending"
    },
    {
      "phase": 5,
      "name": "Hardware Decision Point",
      "description": "Synthesize learnings from Tracks A-C. Assess what experiments were compute-limited, what directions require more resources, whether hardware investment is justified.",
      "features": ["FEAT-017"],
      "entry_criteria": "Phases 2-4 complete",
      "exit_criteria": "Hardware decision document with clear rationale based on experimental evidence",
      "status": "pending"
    },
    {
      "phase": 6,
      "name": "Track D: Paper Reproduction",
      "description": "Target 'The Bayesian Geometry of Transformer Attention'. Read, annotate, summarize in own words, and reproduce/test specific claims from the paper.",
      "features": ["FEAT-018", "FEAT-019", "FEAT-020"],
      "entry_criteria": "Phases 2-4 complete, mental model sufficient to engage with paper",
      "exit_criteria": "Paper summary document, at least one claim reproduced/tested, retrospective reflection on full learning journey",
      "status": "pending"
    }
  ],

  "features": [
    {
      "id": "FEAT-001",
      "name": "Codebase Restructure",
      "phase": 1,
      "description": "Refactor qwen-finetune/ to be explicitly learning-focused. Rename/reorganize to reflect experimental purpose rather than production pipeline.",
      "priority": 1,
      "passes": true,
      "criteria": {
        "functional": [
          "Directory structure reflects learning focus (e.g., experiments/ not scripts/)",
          "Existing QLoRA training still works after restructure",
          "README updated to reflect learning lab purpose"
        ],
        "property": [
          "No production-focused artifacts remain without learning annotations"
        ]
      },
      "verification_commands": ["python -c \"import experiments\" || echo 'structure check'", "ls docs/concepts/"]
    },
    {
      "id": "FEAT-002",
      "name": "Documentation Framework",
      "phase": 1,
      "description": "Create docs/concepts/ directory structure for capturing learnings. Establish template for concept docs.",
      "priority": 2,
      "passes": true,
      "criteria": {
        "functional": [
          "docs/concepts/ directory exists",
          "Template or example concept doc created",
          "Structure supports incremental knowledge capture"
        ]
      },
      "verification_commands": ["test -d docs/concepts && echo 'exists'"]
    },
    {
      "id": "FEAT-003",
      "name": "Questions Log",
      "phase": 1,
      "description": "Create QUESTIONS.md as central questions log. Establish convention for per-experiment questions.",
      "priority": 3,
      "passes": true,
      "criteria": {
        "functional": [
          "QUESTIONS.md exists with initial open questions from VISION.md",
          "Format supports tracking question status (open/answered/superseded)",
          "Per-experiment question convention documented"
        ]
      },
      "verification_commands": ["test -f QUESTIONS.md && echo 'exists'"]
    },
    {
      "id": "FEAT-004",
      "name": "Experiment Template",
      "phase": 1,
      "description": "Create template for instrumented experiments: hypothesis section, code with inline comments, results section, learnings section.",
      "priority": 4,
      "passes": true,
      "criteria": {
        "functional": [
          "Experiment template exists (Python file or notebook structure)",
          "Template includes: hypothesis, methodology, results, learnings sections",
          "First experiment converted to template format"
        ]
      },
      "verification_commands": ["grep -l 'hypothesis' experiments/*.py || echo 'check template'"]
    },
    {
      "id": "FEAT-005",
      "name": "Loss Curve Analysis Experiment",
      "phase": 2,
      "description": "Instrumented experiment to understand what loss curves tell us. Log detailed metrics, visualize, document interpretation.",
      "priority": 1,
      "passes": true,
      "criteria": {
        "functional": [
          "Training script logs loss at configurable granularity",
          "Visualization of loss curve generated",
          "Document explaining what different loss patterns indicate"
        ],
        "theory": [
          "Can explain relationship between loss and model learning"
        ]
      },
      "verification_commands": ["test -f docs/concepts/loss-curves.md"]
    },
    {
      "id": "FEAT-006",
      "name": "Learning Rate Exploration",
      "phase": 2,
      "description": "Experiment varying learning rates to observe effects on training dynamics and final model behavior.",
      "priority": 2,
      "passes": true,
      "criteria": {
        "functional": [
          "Multiple training runs with different learning rates completed",
          "Comparison visualization created",
          "Documented observations on learning rate effects"
        ],
        "property": [
          "Hypothesis documented before running experiments"
        ]
      },
      "verification_commands": ["test -f experiments/learning_rate/README.md"]
    },
    {
      "id": "FEAT-007",
      "name": "LoRA Rank Comparison",
      "phase": 2,
      "description": "Compare different LoRA ranks (8, 16, 32, 64) on training dynamics and model behavior.",
      "priority": 3,
      "passes": true,
      "criteria": {
        "functional": [
          "Training runs with multiple LoRA ranks completed",
          "Parameter count vs performance tradeoff documented",
          "Observations on what rank captures"
        ],
        "theory": [
          "Can explain why low-rank adaptation works conceptually"
        ]
      },
      "verification_commands": ["test -f experiments/lora_rank/README.md"]
    },
    {
      "id": "FEAT-008",
      "name": "Catastrophic Forgetting Test",
      "phase": 2,
      "description": "Test whether fine-tuned model loses general capabilities. Design forgetting benchmark and measure.",
      "priority": 4,
      "passes": true,
      "criteria": {
        "functional": [
          "Forgetting test suite created (math, reasoning, code tasks)",
          "Base vs fine-tuned comparison completed",
          "Results documented with interpretation"
        ]
      },
      "verification_commands": ["test -f experiments/forgetting/results.md"]
    },
    {
      "id": "FEAT-009",
      "name": "Training Dynamics Self-Assessment",
      "phase": 2,
      "description": "Write self-assessment document explaining training dynamics concepts in own words. Answer 'minimum interesting experiment' question.",
      "priority": 5,
      "passes": true,
      "criteria": {
        "functional": [
          "Self-assessment doc covers: forward/backward pass, loss landscapes, optimizer state, batch size/LR interaction",
          "Minimum interesting experiment question answered with rationale",
          "Track A retrospective reflection written"
        ]
      },
      "verification_commands": ["test -f docs/concepts/training-dynamics-self-assessment.md"]
    },
    {
      "id": "FEAT-010",
      "name": "Attention Extraction Tooling",
      "phase": 3,
      "description": "Build or integrate tooling to extract attention weights from model forward passes.",
      "priority": 1,
      "passes": true,
      "criteria": {
        "functional": [
          "Can extract attention weights from any layer",
          "Works with Qwen architecture",
          "Inline documentation explains attention tensor shapes"
        ]
      },
      "verification_commands": ["python -c \"from experiments.attention import extract_attention; print('works')\""]
    },
    {
      "id": "FEAT-011",
      "name": "Attention Visualization",
      "phase": 3,
      "description": "Create visualizations of attention patterns: heatmaps, head comparisons, layer progressions.",
      "priority": 2,
      "passes": true,
      "criteria": {
        "functional": [
          "Attention heatmap generation works",
          "Can compare attention across heads and layers",
          "Visualizations saved with interpretive captions"
        ]
      },
      "verification_commands": ["test -d experiments/attention/visualizations"]
    },
    {
      "id": "FEAT-012",
      "name": "Attention Comparison Experiment",
      "phase": 3,
      "description": "Compare attention patterns between base and fine-tuned models. Document what changes.",
      "priority": 3,
      "passes": true,
      "criteria": {
        "functional": [
          "Side-by-side attention comparisons generated",
          "Changes in attention patterns documented",
          "Hypothesis about what changes mean"
        ]
      },
      "verification_commands": ["test -f experiments/attention/base_vs_finetuned.md"]
    },
    {
      "id": "FEAT-013",
      "name": "Attention Mechanics Self-Assessment",
      "phase": 3,
      "description": "Write self-assessment on attention mechanics. Track B retrospective reflection.",
      "priority": 4,
      "passes": true,
      "criteria": {
        "functional": [
          "Self-assessment covers: QKV computation, attention scores, multi-head attention, positional encoding interaction",
          "Track B retrospective written"
        ]
      },
      "verification_commands": ["test -f docs/concepts/attention-self-assessment.md"]
    },
    {
      "id": "FEAT-014",
      "name": "Activation Extraction Tooling",
      "phase": 4,
      "description": "Build tooling to extract intermediate activations (residual stream values) from model.",
      "priority": 1,
      "passes": false,
      "criteria": {
        "functional": [
          "Can extract activations from any layer",
          "Can extract pre/post attention and FFN activations",
          "Documentation explains residual stream concept"
        ]
      },
      "verification_commands": ["python -c \"from experiments.probing import extract_activations; print('works')\""]
    },
    {
      "id": "FEAT-015",
      "name": "Representation Analysis Experiments",
      "phase": 4,
      "description": "Analyze what information is encoded in activations. Layer-by-layer transformation tracking.",
      "priority": 2,
      "passes": false,
      "criteria": {
        "functional": [
          "Activation statistics computed and visualized",
          "Layer transformation patterns documented",
          "At least one probing experiment completed (e.g., linear probe for some property)"
        ]
      },
      "verification_commands": ["test -f experiments/probing/analysis.md"]
    },
    {
      "id": "FEAT-016",
      "name": "Representations Self-Assessment",
      "phase": 4,
      "description": "Write self-assessment on representations and residual stream. Track C retrospective.",
      "priority": 3,
      "passes": false,
      "criteria": {
        "functional": [
          "Self-assessment covers: residual stream, FFN transformations, how information accumulates",
          "Track C retrospective written"
        ]
      },
      "verification_commands": ["test -f docs/concepts/representations-self-assessment.md"]
    },
    {
      "id": "FEAT-017",
      "name": "Hardware Decision Document",
      "phase": 5,
      "description": "Synthesize Tracks A-C learnings into hardware investment decision with clear rationale.",
      "priority": 1,
      "passes": false,
      "criteria": {
        "functional": [
          "Document identifies compute-limited experiments from Tracks A-C",
          "Document assesses what research directions require more resources",
          "Clear recommendation with rationale (upgrade / don't upgrade / specific upgrade path)"
        ]
      },
      "verification_commands": ["test -f docs/HARDWARE-DECISION.md"]
    },
    {
      "id": "FEAT-018",
      "name": "Paper Annotation and Summary",
      "phase": 6,
      "description": "Read 'Bayesian Geometry of Transformer Attention', create annotated reading notes, write summary in own words.",
      "priority": 1,
      "passes": false,
      "criteria": {
        "functional": [
          "Annotated reading notes exist",
          "Summary document explains paper's core argument in own words",
          "Key claims identified for potential reproduction"
        ]
      },
      "verification_commands": ["test -f docs/papers/bayesian-geometry-attention.md"]
    },
    {
      "id": "FEAT-019",
      "name": "Paper Claim Reproduction",
      "phase": 6,
      "description": "Select and reproduce/test at least one specific claim from the paper.",
      "priority": 2,
      "passes": false,
      "criteria": {
        "functional": [
          "Claim selected with rationale for selection",
          "Reproduction experiment designed and executed",
          "Results documented with comparison to paper claims"
        ]
      },
      "verification_commands": ["test -f experiments/paper_reproduction/bayesian_geometry/results.md"]
    },
    {
      "id": "FEAT-020",
      "name": "Final Retrospective",
      "phase": 6,
      "description": "Write final retrospective on full learning journey. Assess success criteria achievement.",
      "priority": 3,
      "passes": false,
      "criteria": {
        "functional": [
          "Retrospective covers all four tracks",
          "Self-assessment against original success criteria",
          "Open questions for future exploration identified",
          "QUESTIONS.md updated with final status"
        ]
      },
      "verification_commands": ["test -f docs/RETROSPECTIVE.md"]
    }
  ],

  "architectural_decisions": {
    "resolved": [
      {
        "decision": "Codebase organization",
        "choice": "Refactor qwen-finetune/ in place to be learning-focused",
        "rationale": "Builds on existing working code rather than starting fresh",
        "source": "interview"
      },
      {
        "decision": "Documentation format",
        "choice": "Markdown files in docs/concepts/ plus inline Python comments",
        "rationale": "User preference; keeps code and docs separate but both rich",
        "source": "interview"
      },
      {
        "decision": "Questions tracking",
        "choice": "Central QUESTIONS.md plus per-experiment questions sections",
        "rationale": "Allows both overview and contextual question tracking",
        "source": "interview"
      },
      {
        "decision": "Track execution order",
        "choice": "A (fine-tuning) -> B (attention) -> C (probing) -> D (paper)",
        "rationale": "Builds foundational intuition before deeper mechanics and paper work",
        "source": "interview"
      },
      {
        "decision": "Hardware decision timing",
        "choice": "After Tracks B and C complete, before Track D",
        "rationale": "Enough data points to make informed decision, but before most compute-intensive work",
        "source": "interview"
      },
      {
        "decision": "Target paper for Track D",
        "choice": "The Bayesian Geometry of Transformer Attention (arxiv:2512.22471)",
        "rationale": "Both the fluency benchmark and explicit target; mentioned multiple times in vision",
        "source": "vision"
      }
    ],
    "pending_adrs": [
      {
        "decision": "Attention visualization library",
        "options": ["bertviz", "custom matplotlib", "transformers-interpret", "ecco"],
        "decide_in_phase": 3
      },
      {
        "decision": "Activation extraction approach",
        "options": ["hooks", "TransformerLens", "custom wrapper"],
        "decide_in_phase": 4
      }
    ]
  },

  "escalation_triggers": [
    "VRAM exhaustion that prevents running 7B model experiments",
    "Qwen architecture incompatibility with chosen visualization/probing tools",
    "Paper claims require compute beyond available hardware to test",
    "Fundamental confusion about concepts that blocks progress for >2 iterations",
    "Experiments produce results that contradict documented understanding (may indicate bug or learning opportunity)"
  ],

  "loop_parameters": {
    "progress_tracking": ["git_commits", "progress_file", "prd_status_updates"],
    "on_phase_complete": "ask",
    "thoroughness": "high",
    "documentation_priority": "thorough"
  },

  "verification_strategy": {
    "mental_model": "Self-assessment docs reviewed with others/LLMs",
    "paper_fluency": "Reproduce claims + summary in own words",
    "experimental_intuition": "Retrospective reflection docs after each track",
    "per_experiment": "Hypothesis before + learnings after + automated checks where possible"
  }
}
