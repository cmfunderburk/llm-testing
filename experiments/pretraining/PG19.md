# PG-19 Dataset Guide

## What is PG-19?

PG-19 is a large-scale language modeling benchmark from DeepMind, containing ~29,000 books from Project Gutenberg published before 1919. It's designed for training and evaluating long-range language models.

**Key characteristics:**
- ~11GB of text (training set)
- ~28,602 training books, 50 validation books, 100 test books
- Average document length: ~69,000 words (20x longer than WikiText)
- Public domain literature: novels, non-fiction, poetry, plays
- Dated linguistic style (pre-1919 English)

**Paper:** [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)

## Files in This Project

After downloading, you'll have:

| File | Size | Contents |
|------|------|----------|
| `pg19_train.txt` | ~11 GB | 28,602 books |
| `pg19_validation.txt` | ~17 MB | 50 books |
| `pg19_test.txt` | ~40 MB | 100 books |
| `pg19_train_small.txt` | ~40 MB | 100 books (subset) |
| `pg19_validation_small.txt` | ~17 MB | 50 books (same as full) |
| `pg19_test_small.txt` | ~40 MB | 100 books (same as full) |
| `pg19_train_docs.jsonl` | ~11 GB | Full train split, one doc per line |
| `pg19_validation_docs.jsonl` | ~17 MB | Full val split, one doc per line |
| `pg19_test_docs.jsonl` | ~40 MB | Full test split, one doc per line |
| `pg19_train_small_docs.jsonl` | ~40 MB | Small train split, one doc per line |
| `pg19_validation_small_docs.jsonl` | ~17 MB | Small val split, one doc per line |
| `pg19_test_small_docs.jsonl` | ~40 MB | Small test split, one doc per line |
| `pg19_train_normalized.txt` | ~11 GB | Full train split with prose unwrap |
| `pg19_validation_normalized.txt` | ~17 MB | Full val split with prose unwrap |
| `pg19_test_normalized.txt` | ~40 MB | Full test split with prose unwrap |
| `pg19_train_small_normalized.txt` | ~40 MB | Small train split with prose unwrap |
| `pg19_validation_small_normalized.txt` | ~17 MB | Small val split with prose unwrap |
| `pg19_test_small_normalized.txt` | ~40 MB | Small test split with prose unwrap |
| `pg19_train_docs_normalized.jsonl` | ~11 GB | Full docs split with prose unwrap |
| `pg19_validation_docs_normalized.jsonl` | ~17 MB | Full docs val split with prose unwrap |
| `pg19_test_docs_normalized.jsonl` | ~40 MB | Full docs test split with prose unwrap |
| `pg19_train_small_docs_normalized.jsonl` | ~40 MB | Small docs train split with prose unwrap |
| `pg19_validation_small_docs_normalized.jsonl` | ~17 MB | Small docs val split with prose unwrap |
| `pg19_test_small_docs_normalized.jsonl` | ~40 MB | Small docs test split with prose unwrap |

## Downloading

```bash
# Download small subset first (for testing)
uv run python -m experiments.pretraining.download_corpora pg19_small

# Download full dataset (~11GB, takes 30-60 minutes)
uv run python -m experiments.pretraining.download_corpora pg19

# Download document-JSONL variant (recommended for boundary-safe tokenization)
uv run python -m experiments.pretraining.download_corpora pg19_docs
```

Notes:
- `pg19_docs` download is resumable. If interrupted, re-run the same command and it will continue from the existing JSONL file.
- Progress for docs download is periodic and may pause between updates if individual books are slow to fetch.

## Optional: Normalize Prose Line Breaks

Project Gutenberg formatting often includes hard-wrapped prose lines. If you
want more natural paragraph flow during training, generate normalized variants:

```bash
# Normalize small split first (quick sanity check)
uv run python -m experiments.pretraining.prepare_pg19_normalized pg19_small

# Normalize full split
uv run python -m experiments.pretraining.prepare_pg19_normalized pg19

# Normalize document-JSONL full split (preserves exact book boundaries)
uv run python -m experiments.pretraining.prepare_pg19_docs_normalized pg19_docs
```

The normalizer unwraps likely prose blocks while preserving likely verse/list
formatting (poems, headings, structured blocks).

## Using in the Dashboard

Select from the Dataset dropdown:
- **PG-19 (small)** - 100 training books, good for testing the pipeline
- **PG-19 (full)** - Full 28K books, for serious training runs
- **PG-19 (small, normalized)** - same books, prose line-wrap normalized
- **PG-19 (full, normalized)** - same books, prose line-wrap normalized
- **PG-19 (small, docs+EOT)** - JSONL docs with explicit book boundaries
- **PG-19 (full, docs+EOT)** - JSONL docs with explicit book boundaries
- **PG-19 (small, docs+EOT normalized)** - normalized JSONL docs, explicit boundaries
- **PG-19 (full, docs+EOT normalized)** - normalized JSONL docs, explicit boundaries

The dashboard automatically pairs training corpus with official validation split.

## Using via API

```python
# Start training with official splits
POST /api/pretraining/start
{
    "config_name": "nano",
    "corpus": "pg19_train_docs_normalized",
    "val_corpus": "pg19_validation_docs_normalized",
    "epochs": 1,
    "batch_size": 8
}
```

## Recommended Professional Pipeline

For long-running PG-19 experiments where boundary handling matters:

```bash
# 1) Re-acquire with explicit doc boundaries
uv run python -m experiments.pretraining.download_corpora pg19_docs

# 2) Normalize per document (not as one monolithic text)
uv run python -m experiments.pretraining.prepare_pg19_docs_normalized pg19_docs

# 3) Train with *_docs_normalized corpora so tokenization inserts EOT between books
#    (handled automatically by the data pipeline)
```

## Practical Considerations for This Project

### Hardware Context
- **Your GPU:** RTX 5060 Ti 16GB
- **Full PG-19 tokenized:** ~3 billion tokens (GPT-2 tokenizer)
- **Token cache:** ~12GB on disk after first tokenization

### What PG-19 is Good For (in this learning context)

1. **Understanding scale effects**
   - Compare loss curves: tiny corpus vs. TinyStories vs. PG-19
   - See how more data affects convergence and generalization
   - Observe validation loss behavior with proper train/val separation

2. **Long-context experiments**
   - PG-19 books are long, coherent documents
   - Good for testing context length effects (256 → 512 → 1024)
   - Can observe if model learns document-level coherence

3. **Epoch vs. step thinking**
   - With small corpora, you run many epochs (data repetition)
   - With PG-19, even 1 epoch = millions of steps
   - Different learning dynamics emerge

4. **Realistic training time intuition**
   - Feel the difference between "toy" and "real" datasets
   - Understand why larger models/datasets require patience

### What PG-19 is NOT Good For

1. **Modern language generation**
   - Pre-1919 English has different vocabulary, style, idioms
   - Not suitable for chatbots or modern text applications
   - DeepMind explicitly warns against using it for production systems

2. **Quick iteration**
   - First tokenization: 10-30 minutes (11GB file)
   - One epoch with nano model: hours
   - Not for rapid hypothesis testing

3. **Diverse domain coverage**
   - All public domain literature (no technical, scientific, code)
   - Biased toward Western literary canon
   - Limited genre diversity compared to modern web corpora

### Recommended Workflow

1. **Prototype with small datasets:**
   ```
   verdict → shakespeare → tinystories → pg19_small
   ```

2. **Only use full PG-19 when:**
   - You want to observe multi-hour training dynamics
   - You're specifically studying scale effects
   - You want proper train/val/test separation for evaluation

3. **Practical experiment ideas:**
   - Train nano model for 1 epoch on PG-19 vs. 100 epochs on TinyStories
   - Compare validation loss trajectories
   - Generate samples and compare linguistic style

### Tokenization and Caching

First load of PG-19 requires tokenizing the full 11GB file:
- **Time:** 10-30 minutes depending on CPU
- **Output:** Cached to `.cache/pg19_train.gpt2.tokens.pt` (~12GB)
- **Subsequent loads:** Seconds (loads from cache)

The cache is invalidated if you modify the corpus file.

### Memory Considerations

| Phase | RAM Usage | Notes |
|-------|-----------|-------|
| Tokenization | ~15GB peak | Text + tokens in memory |
| Training | ~4GB | DataLoader streams from tokenized cache |
| GPU (nano) | ~3-4GB | Model + gradients + activations |

If tokenization causes OOM, consider:
- Closing other applications
- Using pg19_small for initial experiments

## Comparison with Other Corpora

| Corpus | Size | Tokens | Style | Use Case |
|--------|------|--------|-------|----------|
| verdict | 8 KB | ~2K | Legal | Sanity check |
| shakespeare | 1 MB | ~300K | Elizabethan | Quick experiments |
| wikitext2 | 13 MB | ~2M | Wikipedia | Standard benchmark |
| tinystories | 1.8 GB | ~500M | Modern children's | Medium-scale training |
| pg19_train | 11 GB | ~3B | Pre-1919 lit | Large-scale training |

## Questions to Explore with PG-19

1. How does validation loss behave when train and val are truly separate documents?
2. Does the model learn different patterns from 19th-century vs. modern text?
3. How many tokens does a nano model need to see before loss plateaus?
4. What happens to generated text quality as training progresses?
5. How does context length affect perplexity on long documents?

## References

- [PG-19 GitHub](https://github.com/google-deepmind/pg19)
- [Project Gutenberg](https://www.gutenberg.org/)
- [Compressive Transformers Paper](https://arxiv.org/abs/1911.05507)
