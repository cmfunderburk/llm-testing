"""
Corpus Download Script

Downloads training corpora from HuggingFace datasets.
Available corpora:
- tinystories: 2.1M synthetic short stories (TinyStories dataset)
- wikitext2: Wikipedia articles from WikiText-2
- shakespeare: Complete works of Shakespeare (~1M characters)

Usage:
    python -m experiments.pretraining.download_corpora           # Download all
    python -m experiments.pretraining.download_corpora tinystories
    python -m experiments.pretraining.download_corpora wikitext2
    python -m experiments.pretraining.download_corpora shakespeare
"""

import argparse
import sys
from pathlib import Path

CORPUS_DIR = Path(__file__).parent / "corpus"


def download_tinystories(verbose: bool = True) -> Path:
    """
    Download the TinyStories dataset.

    TinyStories contains 2.1M synthetic short stories generated by GPT-3.5/4,
    designed for training small language models. From the paper:
    "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"

    Returns:
        Path to the downloaded corpus file
    """
    from datasets import load_dataset

    output_path = CORPUS_DIR / "tinystories.txt"

    if output_path.exists():
        if verbose:
            print(f"TinyStories already exists at {output_path}")
            size_mb = output_path.stat().st_size / (1024 * 1024)
            print(f"  Size: {size_mb:.1f} MB")
        return output_path

    if verbose:
        print("Downloading TinyStories from HuggingFace...")
        print("  Dataset: roneneldan/TinyStories")
        print("  This may take a few minutes...")

    dataset = load_dataset("roneneldan/TinyStories", split="train")

    if verbose:
        print(f"  Downloaded {len(dataset):,} stories")
        print(f"  Writing to {output_path}...")

    CORPUS_DIR.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        for i, example in enumerate(dataset):
            f.write(example["text"])
            f.write("\n\n")  # Separate stories with blank line

            if verbose and (i + 1) % 500000 == 0:
                print(f"    Wrote {i + 1:,} stories...")

    size_mb = output_path.stat().st_size / (1024 * 1024)
    if verbose:
        print(f"  Done! Size: {size_mb:.1f} MB")

    return output_path


def download_wikitext2(verbose: bool = True) -> Path:
    """
    Download the WikiText-2 dataset.

    WikiText-2 contains ~2M tokens from Wikipedia's "Good" and "Featured" articles.
    It's a standard benchmark for language modeling.

    Returns:
        Path to the downloaded corpus file
    """
    from datasets import load_dataset

    output_path = CORPUS_DIR / "wikitext2.txt"

    if output_path.exists():
        if verbose:
            print(f"WikiText-2 already exists at {output_path}")
            size_mb = output_path.stat().st_size / (1024 * 1024)
            print(f"  Size: {size_mb:.1f} MB")
        return output_path

    if verbose:
        print("Downloading WikiText-2 from HuggingFace...")
        print("  Dataset: wikitext/wikitext-2-raw-v1")

    dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

    CORPUS_DIR.mkdir(parents=True, exist_ok=True)

    if verbose:
        print(f"  Writing to {output_path}...")

    with open(output_path, "w", encoding="utf-8") as f:
        # Combine train, validation, and test splits
        for split_name in ["train", "validation", "test"]:
            split = dataset[split_name]
            for example in split:
                text = example["text"].strip()
                if text:  # Skip empty lines
                    f.write(text)
                    f.write("\n")

    size_mb = output_path.stat().st_size / (1024 * 1024)
    if verbose:
        print(f"  Done! Size: {size_mb:.1f} MB")

    return output_path


def download_shakespeare(verbose: bool = True) -> Path:
    """
    Download the Tiny Shakespeare dataset.

    Contains the complete works of Shakespeare (~1.1M characters).
    Originally from Andrej Karpathy's char-rnn project.

    Returns:
        Path to the downloaded corpus file
    """
    import urllib.request

    output_path = CORPUS_DIR / "shakespeare.txt"

    if output_path.exists():
        if verbose:
            print(f"Shakespeare already exists at {output_path}")
            size_kb = output_path.stat().st_size / 1024
            print(f"  Size: {size_kb:.1f} KB")
        return output_path

    url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"

    if verbose:
        print("Downloading Tiny Shakespeare...")
        print(f"  URL: {url}")

    CORPUS_DIR.mkdir(parents=True, exist_ok=True)

    urllib.request.urlretrieve(url, output_path)

    size_kb = output_path.stat().st_size / 1024
    if verbose:
        print(f"  Done! Size: {size_kb:.1f} KB")

    return output_path


DOWNLOADERS = {
    "tinystories": download_tinystories,
    "wikitext2": download_wikitext2,
    "shakespeare": download_shakespeare,
}


def download_all(verbose: bool = True):
    """Download all available corpora."""
    print("=" * 60)
    print("Downloading all corpora")
    print("=" * 60)

    for name, downloader in DOWNLOADERS.items():
        print(f"\n[{name}]")
        try:
            downloader(verbose=verbose)
        except Exception as e:
            print(f"  ERROR: {e}")

    print("\n" + "=" * 60)
    print("Download complete!")
    print("=" * 60)


def list_corpora():
    """List all corpora and their status."""
    print("Available corpora:")
    print("-" * 40)

    for name in DOWNLOADERS.keys():
        path = CORPUS_DIR / f"{name}.txt"
        if path.exists():
            size_mb = path.stat().st_size / (1024 * 1024)
            status = f"Downloaded ({size_mb:.1f} MB)"
        else:
            status = "Not downloaded"
        print(f"  {name}: {status}")

    # Also show built-in corpora
    print("\nBuilt-in corpora:")
    print("-" * 40)
    for name in ["verdict", "tiny"]:
        path = CORPUS_DIR / f"{name}.txt"
        if path.exists():
            size_kb = path.stat().st_size / 1024
            print(f"  {name}: {size_kb:.1f} KB")
        else:
            print(f"  {name}: Missing")


def main():
    parser = argparse.ArgumentParser(
        description="Download training corpora for GPT pretraining",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python -m experiments.pretraining.download_corpora --list
    python -m experiments.pretraining.download_corpora tinystories
    python -m experiments.pretraining.download_corpora wikitext2
    python -m experiments.pretraining.download_corpora --all
        """
    )

    parser.add_argument(
        "corpus",
        nargs="?",
        choices=list(DOWNLOADERS.keys()),
        help="Corpus to download"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Download all corpora"
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="List available corpora and their status"
    )

    args = parser.parse_args()

    if args.list:
        list_corpora()
        return

    if args.all:
        download_all()
        return

    if args.corpus:
        downloader = DOWNLOADERS[args.corpus]
        downloader(verbose=True)
        return

    # No arguments - show help
    parser.print_help()


if __name__ == "__main__":
    main()
